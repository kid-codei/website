---
title: "project2_jec4968"
author: "John Henry Cruz"
date: "11/24/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Project 2

```{r}
library(readr)
library(tidyverse)
library(tidyr)
library(knitr)
library(dplyr)
library(ggplot2)
library(lmtest)
library(sandwich)
library(MASS)
library(plotROC)
library(glmnet)

```

## 0) My Dataset
```{r}
combined_data <- readr::read_csv("https://raw.githubusercontent.com/5harad/openpolicing/master/results/data_for_figures/combined_data.csv")

open_policing <- combined_data %>% dplyr::select(1:8) %>%na.omit() 

open_policing_binary <- open_policing %>% mutate(consent_search_rate = case_when(consent_search_rate == 0 ~ 0, TRUE ~ 1)) %>% rename('consent_search_bin' = consent_search_rate)

open_policing <- left_join(open_policing, open_policing_binary)

open_policing_binary_2 <- open_policing %>% mutate(consent_search_rate = case_when(consent_search_rate == 0 ~ 'no', TRUE ~ 'yes')) %>% rename('consent_search_cat' = consent_search_rate)

open_policing <- left_join(open_policing, open_policing_binary_2)
```

#### The combined_data dataset is based off the Stanford Open Policing Project. This dataset was created by multiple researchers that wanted to create a repository detailing interactions between the police and the public. Their compiled dataset has over 2 million stops, so  this dataset from GitHub is a small part of it. I then filtered the dataset into fewer variables that I was more interested in and removed NA's. This made the dataset work so that both categorical variables had 2-5 groups and all numerical variables had no NA's, except the location variable because that has overlaps that needed to show to understand the data. I removed 3 numerical variables because they were lacking a great amount of data across multiple states and multiple driver races. I also wanted to make sure that the driver races in each numerical variable were complete in the sense that the specific location in that state had a complete set of driver_race(White, Black, and Hispanic). Since the data is somewhat grouped by location, I wanted to make sure that this was consistent, and chose the numerical variables that would allow this to happen. Since the consent_search_rate variable had low numerical values, I decided to make those that were 0 to 0 and any instance of consent_search_rate to 1, and make a new binary variable that would be used later in the project. The varibles are location (the county that the stop happened in) state (where the traffic stop happened) driver_race (the race of the driver), stops_per_year(the number of stops for that race in that state in that location in a year), stop_rate (the percentage that that race in that state in that location gets stopped), search_rate (the percentage that that race in that state in that location gets searched), consent_search_rate (the percentage that that race in that state in that location gets searched with consent), and arrest_rate (the percentage that that race in that state in that location gets arrested).

## 1) MANOVA Testing

### MANOVA
```{r}
manova_data <- manova(cbind(stops_per_year, stop_rate, search_rate, consent_search_rate, arrest_rate)~driver_race, data=combined_data)

summary(manova_data)

```

### ANOVA
```{r}
summary.aov(manova_data)
```

### Means of Each Group Across the Numerical Variables
```{r}
open_policing%>%group_by(driver_race)%>%summarize(mean(stops_per_year),mean(stop_rate),mean(search_rate),mean(consent_search_rate),mean(arrest_rate))

```

### Post Hoc test for Stops per Year
```{r}
pairwise.t.test(open_policing$stops_per_year,open_policing$driver_race,
p.adj="none")
```

### Post Hoc test for Stop Rate
```{r}
pairwise.t.test(open_policing$stop_rate,open_policing$driver_race,
p.adj="none")
```

### Post Hoc test for Search Rate
```{r}
pairwise.t.test(open_policing$search_rate,open_policing$driver_race,
p.adj="none")
```

### Post Hoc test for Consent Search Rate
```{r}
pairwise.t.test(open_policing$consent_search_rate,open_policing$driver_race,
p.adj="none")
```

### Post Hoc test for Arrest Rate
```{r}
pairwise.t.test(open_policing$arrest_rate,open_policing$driver_race,
p.adj="none")
```

### Discussion 

#### In total, 21 tests were performed; 1 MANOVA, 5 ANOVA, and 15 post hoc t tests. 0.05/21 = 0.002380952, which is the new level of significance that we will be looking at.

#### The MANOVA test results show significance(p val = 2.2e-16), so we should look at individual ANOVA's for each numerical variable against driver_race to look for significance between those groups. 

#### An ANOVA test for each of the 5 numerical variables showed significant except one variable, stop_rate. 3 post hoc tests where then ran for each of the 5 numerical variables tested to see which groups are significant whithin that numerical variable. But we only look at 4 of the numerical variables since one of them is not statistically significant.

#### In stops_per_year, there was significance seem between Whites and Blacks and Whites and Hispanics. 

#### In search_rate, there was significance seem between Whites and Blacks and Whites and Hispanics. 

#### In consent_search_rate, there was significance seem between Whites and Blacks and Whites and Hispanics. 

#### In arrest_rate, there was significance seem between Whites and Blacks and Whites and Hispanics and Blacks and Hispanics. 

#### In regards to the assumptions of the MANOVA, they most likely have been met, but there is a possibility that they weren't. This is because the data that was collected by the project was a compilation of all the data that the project could get, and not random samples within all the traffic stops that were recorded by the state. The results of the post hoc tests do show that a lot of the results show significance of the numerical variables towards Whites versus Blacks and Hispanics which may show that there is some unnormality in the data. But there doesn't seem to be any extreme outliers or extreme multicollinearity between any of the variables. 

## 2) Randomization Test

### Finding Mean of Distribution
```{r}
open_policing%>%group_by(consent_search_bin)%>%summarize(m=mean(arrest_rate))%>%summarize(diff(m))

```

### Random Distribution and p value Result
```{r}
rand_dist<-vector()
for(i in 1:5000){
new<-data.frame(arrestrate=sample(open_policing$arrest_rate),consentsearchratebin=open_policing$consent_search_bin)
rand_dist[i]<-mean(new[new$consentsearchratebin=="1",]$arrestrate)-
mean(new[new$consentsearchratebin=="0",]$arrestrate)
}

mean(rand_dist > 0.005091409)*2 #pvalue

```

### Visualization of the Random Distribution
```{r}
{hist(rand_dist,main="",ylab=""); abline(v = 0.005091409	,col="red")}
```

### Discussion

#### I wanted to see if there was a statistical significance between arrest rate and consent to search rate. I used the binary version of the consent to stop rate to run this randomization test.

#### Ho : mean arrest rate to of drivers is the same for consent vs. non-consent searches
#### Ha : mean arrest rate to of drivers is different for consent vs. non-consent searches

#### The results of the randomization tests show that the model is mostly normal with a slight skew towards the lower values. There is a great enough difference in the means that shows that there is a significant difference between the arrest rates of drivers that had or didn't have consented searches. This was verified with the p value of 0.0328 which is < 0.05. The line of the mean in the histogram(shown in red) also shows the mean far away from the mean of the distribution, further arguing for the significance. 

## 3) Linear Regression Model

### Interpret Coefficient Estimates
```{r}
open_policing$stop_rate_c <- open_policing$stop_rate - mean(open_policing$stop_rate)
fit_3<-lm(arrest_rate ~ consent_search_cat*stop_rate_c, data=open_policing)
summary(fit_3)
```

#### When stop_rate_c increases by 1, on average, the arrest_rate decreases by
0.0011040. When a search is executed with consent, on average, the arrest_rate
increases by 0.0046618. Assuming there is consent to search, the effect of the
stop_rate is lesser by -0.0024267 compared to there being no stop.

### Plot Regression
```{r}
ggplot(open_policing, aes(x=stop_rate, y=arrest_rate,group=consent_search_cat))+geom_point(aes(color=consent_search_cat))+
geom_smooth(method="lm",se=F,fullrange=T,aes(color=consent_search_cat))+
theme(legend.position=c(.9,.19))+xlab("")
```

### Check Assumptions
```{r}
resids<-fit_3$residuals
fitvals<-fit_3$fitted.values
ggplot()+geom_point(aes(fitvals,resids))+geom_hline(yintercept=0, color='red')

ggplot()+geom_histogram(aes(resids), bins=20)

ggplot(open_policing,aes(stop_rate,arrest_rate,color=consent_search_cat))+geom_point()
```

### Regression Results with Robust Standard Error
```{r}
coeftest(fit_3, vcov = vcovHC(fit_3))[,1:2]
```

```{r}
coeftest(fit_3)[,1:2]
```

#### Overall, the standard errors before and after robust adjustments were near identical except for the stop_rate_c variable. The robust adjustment increased the standard error, making the p value for that variable greater, reducing that variables chance of being statistically significant.

### Proportion of Variance in Outcome Explained by Model
```{r}
summary(fit_3)$r.sq
```

#### The proportion of the variation in the outcome that my model explains is 0.01369648.

### Rerun the Regression but without Interactions 
```{r}
fit_4<-lm(arrest_rate ~ consent_search_cat + stop_rate_c, data=open_policing)
summary(fit_4)
```


## 4) Bootstrapped Standard Errors
```{r}
coeftest(fit_3)[,1:2]

samp_distn<-replicate(5000, {
boot_dat<-open_policing[sample(nrow(open_policing),replace=TRUE),]
fit_boot<-lm(arrest_rate ~ consent_search_cat*stop_rate_c,data=boot_dat)
coef(fit_boot)
})
samp_distn%>%t%>%as.data.frame%>%summarize_all(sd)
```

#### All of the standard errors (original, robust, and bootstrap) have extremely similar standard errors for consent_search_catyes and consent_search_catyes:stop_rate_c. The stop_rate_c is the variable that have varying standard errors across the three different standard error measurements. The original standard error had a much lower standard error and the bootstrapped standard error had a greater standard error, which the robust standard error staying around ~ 0.002. The original standard error for stop_rate_c would have a lower p value than the robust and bootstrapped standard errors, and the bootstrapped standard errors would have a higher p value than the robust and original standard errors.

## 5) Logistic Regression Model

### Interpret Coeeficient Estimates
```{r}
fit_5<-glm(consent_search_bin~stops_per_year+stop_rate,data=open_policing,family=binomial(link="logit"))

coeftest(fit_5)

coef(fit_5)%>%round(3)%>%data.frame

exp(coef(fit_5))%>%round(3)%>%data.frame
```

#### With every one unit increase in stop_rate, the odds of having a search with consent multiplies by 0.895. With every one unit increase in stops_per_year, the odds of having a search with consent multiplies by 1.000.

### Report Confusin Matrix
```{r}
prob<-predict(fit_5,type="response")
pred<-ifelse(prob>.5,1,0)
table(truth=open_policing$consent_search_bin, prediction=pred)%>%addmargins
```


### Accuracy, Sensitivity, Specificity, Recall
```{r}
# Accuracy
(396+2)/594

# Sensitivity (TPR)
396/397

# Specificity (TNR)
2/3

# Recall
396/591
```

#### The accucary was seen to be 0.9974811, the sensitivity 0.6700508, the specificity 0.6666667, and the recall 0.6700337. Overall, the sensitivity of the model was really high, but the overall precision and accuracy wasnâ€™t nearly the same.

### Plot Density of log-odds by Binary Outcome Variable
```{r}
pca1<-princomp(open_policing[c('stop_rate','stops_per_year')])
open_policing$predictor<-pca1$scores[,1]

fit_6<-glm(consent_search_bin~predictor,data=open_policing,family="binomial")
open_policing$prob<-predict(fit_6,type="response")

open_policing$logit <- predict(fit_5)

ggplot(open_policing, aes(logit, fill = consent_search_cat)) + geom_density(alpha = 0.3) + geom_vline(xintercept = 0, lty = 2)
```


### ROC Curve and AUC
```{r}
sens<-function(p,data=open_policing, y=consent_search_bin)mean(open_policing[open_policing$consent_search_bin==1,]$prob>p)
spec<-function(p,data=open_policing, y=consent_search_bin)mean(open_policing[open_policing$consent_search_bin==0,]$prob<p)

sensitivity<-sapply(seq(0,1,.01),sens, open_policing)
specificity<-sapply(seq(0,1,.01),spec, open_policing)

ROC1<-data.frame(sensitivity,specificity,cutoff=seq(0,1,.01))

ROC1$TPR<-sensitivity
ROC1$FPR<-1-specificity

ROC1%>%ggplot(aes(FPR,TPR))+geom_path(size=1.5)+geom_segment(aes(x=0,y=0,xend=1,yend=1),
lty=2)+
scale_x_continuous(limits = c(0,1))
```


### C10-fold CV and Report Accuracy, Sensitivity, and Recall
```{r}
class_diag<-function(probs,truth){
tab<-table(factor(probs>.5,levels=c("FALSE","TRUE")),truth)
acc=sum(diag(tab))/sum(tab)
sens=tab[2,2]/colSums(tab)[2]
spec=tab[1,1]/colSums(tab)[1]
ppv=tab[2,2]/rowSums(tab)[2]
if(is.numeric(truth)==FALSE & is.logical(truth)==FALSE) truth<-as.numeric(truth)-1

ord<-order(probs, decreasing=TRUE)
probs <- probs[ord]; truth <- truth[ord]
TPR=cumsum(truth)/max(1,sum(truth))
FPR=cumsum(!truth)/max(1,sum(!truth))
dup<-c(probs[-1]>=probs[-length(probs)], FALSE)
TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
n <- length(TPR)
auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
data.frame(acc,sens,spec,ppv,auc)}
```

```{r}
set.seed(1234)
k=10
data_5<-open_policing[sample(nrow(open_policing)),]
folds<-cut(seq(1:nrow(open_policing)),breaks=k,labels=F)
diags<-NULL
for(i in 1:k){
train_5<-data_5[folds!=i,]
test_5<-data_5[folds==i,]
truth_5<-test_5$consent_search_bin

fit_7<-glm(consent_search_bin~stops_per_year+stop_rate,data=train_5,family="binomial")
probs_5<-predict(fit_7,newdata = test_5,type="response")

diags<-rbind(diags,class_diag(probs_5,truth_5))
}

apply(diags,2,mean)
```


## 6) LASSO Regression
```{r}
open_policing$location <- factor(open_policing$location)
open_policing$state <- factor(open_policing$state)
open_policing$driver_race <- factor(open_policing$driver_race)
open_policing$consent_search_cat <- factor(open_policing$consent_search_cat)

fit_lasso <- glm(consent_search_bin ~ -1 + location + state + driver_race + stops_per_year + stop_rate + search_rate + consent_search_rate + arrest_rate, data = open_policing,
family = "binomial")

model.matrix(fit_lasso) %>% head()
```

```{r}
set.seed(1234)

x<-model.matrix(fit_lasso)
x<-scale(x)
y<-as.matrix(open_policing$consent_search_bin)

cv<-cv.glmnet(x,y,family='binomial')
lasso<-glmnet(x,y,family='binomial',lambda=cv$lambda.1se)
coef(cv)
```

```{r}
set.seed(1234)

data_6<-open_policing[sample(nrow(open_policing)),]
folds_6<-cut(seq(1:nrow(open_policing)),breaks=k,labels=F)

diags<-NULL
for(i in 1:k){
train_6<-data_6[folds_6!=i,]
test_6<-data_6[folds_6==i,]
truth_6<-test_6$consent_search_bin
fit_8<-glm(consent_search_bin~stops_per_year,data=train_6,family="binomial")
probs_6<-predict(fit_8,newdata = test_6,type="response")
preds_6<-ifelse(probs_6>.5,1,0)
diags<-rbind(diags,class_diag(probs_6,truth_6))
}

diags%>%summarize_all(mean)
```

#### Comparing these LASSO results to the out of sample accuracy, the LASSO results had a lower specificity, but had a greater accuracy, sensitivity, precision, and AUC compared to part 5. This means higher proportion of correctly classified cases, higher true positive rate, greater proportion of those who were consented to a search that were correctly predicted, and greater true positive rate according to the AUC.

#### In the LASSO regression, I did not add the variable consent_search_cat as a factor in the final regression. This is because consent_search_cat is the categorical version (yes, no) of the variable consent_search_bin (1,0) and all the results came out to 1/1 when using that as a factor in the regression.
